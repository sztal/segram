{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stories frames and analysis of multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from segram import Corpus, Story\n",
    "\n",
    "# Use GPU acceleration for parsing documents if available\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "# Paths\n",
    "HERE = Path(\".\").absolute()\n",
    "DATA = HERE/\"data\"\n",
    "\n",
    "# Read dataset of articles coveriing the war in Syria\n",
    "data = pd.read_csv(DATA/\"FA-KES-Dataset.csv\", encoding=\"latin\")\n",
    "\n",
    "# Sanitize beginnings of content strings to get rid of publication dates\n",
    "# and other metadata. This is done using a few regular expressions\n",
    "#\n",
    "# Get rid of publication dates\n",
    "pattern1 = r\"^(\" \\\n",
    "    r\"\\s*([A-Z]\\w+)?\\s*\\d{1,2}\\s*([A-Z]\\w+)?\\s*\\d{2,4}\\s*\" \\\n",
    "    r\"|\" \\\n",
    "    r\"\\d{1,2}[-\\.]\\d{1,2}[-\\.]\\d{2,4}\\s*\" \\\n",
    "    r\")\" \\\n",
    "    r\"(\\s*at\\s*\\d+:?\\d+)?\"\n",
    "# Get rid of update dates\n",
    "pattern2 = r\"^\\s*(\\(updated\\s*[\\w\\d]*\\s*\\))\\s*\"\n",
    "# Get rid of get url shout outs\n",
    "pattern3 = r\"^\\s*get\\s*short\\s*url\\s*([\\d\\w]+\\s*)*\"\n",
    "pattern4 = r\"^Date\\s*of\\s*publication\\s*\\d{1,2}\\s*\\w+\\s*\\d{2,4}\\s*\"\n",
    "\n",
    "# Sanitize the content column\n",
    "data[\"article_content\"] = data[\"article_content\"] \\\n",
    "    .str.replace(pattern1, r\"\", regex=True) \\\n",
    "    .str.replace(pattern2, r\"\", regex=True, flags=re.IGNORECASE) \\\n",
    "    .str.replace(pattern3, r\"\", regex=True, flags=re.IGNORECASE) \\\n",
    "    .str.replace(pattern4, r\"\", regex=True, flags=re.IGNORECASE) \\\n",
    "    .str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.add_pipe(\"segram\", config={\n",
    "    \"vectors\": \"en_core_web_lg\"\n",
    "})\n",
    "nlp.add_pipe(\"segram_coref\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fpath = DATA/\"titles.segram\"\n",
    "\n",
    "if fpath.exists():\n",
    "    titles = Corpus.from_disk(fpath, vocab=nlp.vocab, progress=True)\n",
    "else:\n",
    "    texts = data[\"article_title\"]\n",
    "    titles = Corpus.from_texts(nlp, *texts, progress=True, tqdm_kws={\n",
    "        \"total\": len(texts)\n",
    "    })\n",
    "    titles.to_disk(fpath, nlp=False, vocab=False)\n",
    "\n",
    "# Make sure word vectors are stored on CPU after parsing\n",
    "# this is usually desirable when using semantic similarity methods\n",
    "titles = Story(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fpath = DATA/\"articles.segram\"\n",
    "if fpath.exists():\n",
    "    articles = Corpus.from_disk(fpath, vocab=nlp.vocab, progress=True)\n",
    "else:\n",
    "    texts = data[\"article_content\"]\n",
    "    articles = Corpus.from_texts(nlp, *texts, progress=True, tqdm_kws={\n",
    "            \"total\": len(texts)\n",
    "        })\n",
    "    articles.to_disk(fpath, nlp=False, vocab=False)\n",
    "# Make sure word vectors are stored on CPU after parsing\n",
    "# this is usually desirable when using semantic similarity methods\n",
    "articles = Story(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move word vectors to CPU\n",
    "# since this is more optimal for structured similarity methods\n",
    "titles.corpus.ensure_cpu_vectors()\n",
    "articles.corpus.ensure_cpu_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fpath = DATA/\"titles.pkl\"\n",
    "\n",
    "if fpath.exists():\n",
    "    with open(fpath, \"rb\") as fh:\n",
    "        S = pickle.load(fh)\n",
    "else:\n",
    "    story = titles\n",
    "    n_sents = sum(len(doc.smap) for doc in story.docs)\n",
    "    S = np.zeros((n_sents, n_sents), dtype=float)\n",
    "    for i, si in tqdm(enumerate(story.sents), total=n_sents):\n",
    "        for j, sj in enumerate(story.sents[:i]):\n",
    "            S[i, j] = si.similarity(sj, method=\"phrases\")\n",
    "    S += S.T\n",
    "    np.fill_diagonal(S, 1)\n",
    "    S = (S + S.T) / 2\n",
    "    with open(fpath, \"wb\") as fh:\n",
    "        pickle.dump(S, fh)\n",
    "\n",
    "\n",
    "L, V = np.linalg.eigh(S)\n",
    "L = np.diag(np.clip(L, 0, None)[::-1])\n",
    "V = V[:, ::-1]\n",
    "Z = (V@np.sqrt(L))[:, :2]\n",
    "var_explained = L.diagonal() / L.diagonal().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "var = var_explained[:10]\n",
    "ax.plot(\n",
    "    np.arange(1, len(var)+1), var,\n",
    "    marker=\"o\", markersize=10, markeredgecolor=\"black\",\n",
    "    ls=\"--\", lw=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.scatter(*Z.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fpath = DATA/\"articles.pkl\"\n",
    "\n",
    "if fpath.exists():\n",
    "    with open(fpath, \"rb\") as fh:\n",
    "        S = pickle.load(fh)\n",
    "else:\n",
    "    story = articles\n",
    "    n_sents = sum(len(doc.smap) for doc in story.docs)\n",
    "    S = np.zeros((n_sents, n_sents), dtype=float)\n",
    "    for i, si in tqdm(enumerate(story.sents), total=n_sents):\n",
    "        for j, sj in enumerate(story.sents[:i]):\n",
    "            S[i, j] = si.similarity(sj, method=\"phrases\")\n",
    "    S += S.T\n",
    "    np.fill_diagonal(S, 1)\n",
    "    S = (S + S.T) / 2\n",
    "    with open(fpath, \"wb\") as fh:\n",
    "        pickle.dump(S, fh)\n",
    "\n",
    "\n",
    "L, V = np.linalg.eigh(S)\n",
    "L = np.diag(np.clip(L, 0, None)[::-1])\n",
    "V = V[:, ::-1]\n",
    "Z = (V@np.sqrt(L))[:, :2]\n",
    "var_explained = L.diagonal() / L.diagonal().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
